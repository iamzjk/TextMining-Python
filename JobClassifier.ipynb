{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prepare sample data from POSTags, NO longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputfile = \"./tags/nnp-summary.txt\"\n",
    "\n",
    "#df_nn_summary = pd.read_table(nn_summary, sep=\"|\",index_col=False, header=None, error_bad_lines=False)\n",
    "\n",
    "#names = [\"token\"]*1314\n",
    "#df_nn_summary = pd.read_table(nn_summary, sep=\"|\",index_col=False, header=None, names = names) #error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(inputfile) as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    d = list(reader)\n",
    "print d[0][2] # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL|SQL Developer|ETL|SSIS|SQL|Server|SQL|Server|Web|JavaScript|JQuery|HTML5|CSS3|JavaScript|•|Extensive|Microsoft|Business|Intelligence|SQL|Server|Integration|Services|SSIS|SQL|Server|Reporting|Services|SSRS|ETL|SSIS|Custom|Report|Tabular|Reports|Matrix|Reports|Ad|SQL|Server|Reporting|Services|SSRS|DATA|•|Expert|Bulk|BCP|Data|Transformation|Services|DTS|SSIS|•|Expertise|SQL|Server|Data|Modeling|Star|Schema/Snowflake|Schema|Fact|Dimensions|Visio|Software|Development|Life|Cycle|SDLC|Agile|Methodology|Waterfall|ETL|SSIS|•|Extensive|SSIS|File|Transfer|Protocol|FTP|Secure|File|Transfer|Protocol|SFTP\n"
     ]
    }
   ],
   "source": [
    "# the first line of our data\n",
    "print '|'.join(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# TECH category list\n",
    "d_target = [item[0] for item in d]\n",
    "print d_target[0:10]\n",
    "print len(d_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL Developer', 'SQL Developer', 'SQL / BI Developer', 'Sr. SQL Developer', 'Sr. SQL Server DBA', 'Sr. SQL Developer', 'PL/SQL Developer', 'Sr. SQL Server DBA', 'SQL Server DBA', 'Sql Server/bi Developer']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# TITLE category list\n",
    "d_title = [' '.join(item[1:2]) for item in d]\n",
    "print d_title[0:10]\n",
    "print len(d_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ETL', 'SSIS', 'SQL', 'Server', 'SQL', 'Server', 'Web', 'JavaScript', 'JQuery', 'HTML5']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# token list\n",
    "d_train = [item[2:] for item in d]\n",
    "print d_train[0][:10]\n",
    "print len(d_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 [NEW] Read origin text corpus from MongoDB instead of using POSTags\n",
    "files are in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient('192.168.1.232',27017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'movie',\n",
       " u'TaggedResumes',\n",
       " u'OperatingSystems',\n",
       " u'Jobs',\n",
       " u'Mails',\n",
       " u'KeyWords',\n",
       " u'IndeedJobs',\n",
       " u'TaggedMails',\n",
       " u'ResumeLinks',\n",
       " u'Technologies']"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resumes = db.TaggedResumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = resumes.find({},{\"searchKey\": 1, \"title\": 1, \"details.summary\": 1})\n",
    "\n",
    "summary = np.array(list(summary))\n",
    "\n",
    "summary[0].values()#[2].values()\n",
    "\n",
    "for i in range(len(summary)):\n",
    "#summary[0]\n",
    "    summary[i] = summary[i].values()\n",
    "    summary[i][2] = summary[i][2].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071\n"
     ]
    }
   ],
   "source": [
    "print len(summary)\n",
    "#summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 • Over 6 years of Experience in requirements gathering, design, development, implementation and testing the web applications and ETL tools which includes SSIS, SQL Server 2012, and SQL Server 2008. \r\n",
      "• Experience in documenting the business requirements for the development team. \r\n",
      "• Experience in Web design, development, testing/debugging, implementation, and operations in medium-to-large enterprise environments \r\n",
      "• Experienced in JavaScript, JQuery, HTML5, CSS3, and JavaScript. \r\n",
      "• Extensive experience in Microsoft Business Intelligence technologies like SQL Server Integration Services (SSIS) and SQL Server Reporting Services (SSRS). \r\n",
      "• Worked on all activities related to the development, implementation, administration and support of ETL processes for large-scale Data Warehouses using SSIS 2012. \r\n",
      "• Experience in developing Custom Report and different types of Tabular Reports, Matrix Reports, Ad hoc reports and distributed reports in multiple formats using SQL Server Reporting Services (SSRS) in Business intelligence development studio (BIDS)/DATA TOOLS. \r\n",
      "• Expert in using tools like Bulk Copy (BCP), Data Transformation Services (DTS) and SSIS. \r\n",
      "• Expertise in creating Stored Procedures, Functions, Views and Triggers using SQL Server. \r\n",
      "• Experience in Data Modeling using Star Schema/Snowflake Schema, Fact and Dimensions tables, Physical and logical data modeling using Visio. \r\n",
      "• Experience with full Software Development Life Cycle (SDLC). \r\n",
      "• Experience in Agile Methodology and Waterfall models. \r\n",
      "• Experience in working with ETL tools (SSIS). \r\n",
      "• Extensive experience in error handling and problem fixing in SSIS. \r\n",
      "• Worked with File Transfer Protocol (FTP) and Secure File Transfer Protocol (SFTP) to pull or send the files from one server to another server \r\n",
      "• Ability to translate business reporting needs and/or operational reporting requests into technical requirements.\n"
     ]
    }
   ],
   "source": [
    "f_train = [\"\".join(item[2]) for item in summary]\n",
    "print len(f_train), f_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 [u'SQL', u'SQL', u'SQL', u'SQL', u'SQL', u'SQL', u'SQL', u'SQL', u'SQL', u'SQL']\n"
     ]
    }
   ],
   "source": [
    "f_tech = [item[0] for item in summary]\n",
    "print len(f_tech), f_tech[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 [u'SQL Developer', u'SQL Developer', u'SQL / BI Developer', u'Sr. SQL Developer', u'Sr. SQL Server DBA', u'Sr. SQL Developer', u'PL/SQL Developer', u'Sr. SQL Server DBA', u'SQL Server DBA', u'Sql Server/bi Developer']\n"
     ]
    }
   ],
   "source": [
    "f_title = [item[3] for item in summary]\n",
    "print len(f_title), f_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'.NET',\n",
       " u'Agile',\n",
       " u'Automation',\n",
       " u'C#',\n",
       " u'C++',\n",
       " u'CRM',\n",
       " u'CSS',\n",
       " u'Cloud',\n",
       " u'HTML',\n",
       " u'Java',\n",
       " u'JavaScript',\n",
       " u'Linux',\n",
       " u'Microsoft SQL Server',\n",
       " u'Microsoft Visio',\n",
       " u'Microsoft Windows',\n",
       " u'Oracle',\n",
       " u'SDLC',\n",
       " u'SQL',\n",
       " u'Unix',\n",
       " u'Web services',\n",
       " u'Workflow',\n",
       " u'XML'}"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(f_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing input data : job requirement.docx/doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read in raw text into single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Senior Oracle Database Developer  Softweb Solutions Inc - Parsippany, NJ - Must have strong Oracle PL/SQL application development skills and excellent knowledge of Stored Procedures, Functions, Packages, Triggers, Views,Materialized Views, Cursors, and XML features and writing and tuning complex SQL queries. - Additionally, experience with autonomous transactions and error handling is highly desirable. - Experience in Implementing data models and database designs - Experienced with Stored procedure creation/debugging/ optimization/tuning - Experience with UNIX shell and CVS is helpful - Experience with Oracle 12c is desirable - Writing and creating data load scripts and stored procedures; ensuring that data is loaded in a timely manner daily - Working with application developers to assist in database design, query tuning, index assignment, and trigger and procedure creation - Ability to research and trouble-shoot application problems using PL/SQL programming skills - Ensuring that the integrity of company databases is maintained - Actively helping our Data Integrity team reconcile data - Excellent communication skills; works well in a team environment  Required Skills / Competencies: - 5-8+ years Software Development experience - 5-8+ years Designing and developing applications on Oracle RDBMS - 5+ years of experience in complex stored procedure creation and using performance tuning techniques - 4+ years of experience in creating and maintaining SQL code for medium to large OLTP applications  Agile Methodology programming experience is a plus  Job Type: Full-time  Job Location:  Parsippany, NJ Required education:  Bachelor's Required experience:  Oracle PL/SQL: 5 years  \""
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with open('./Python_job_req1.txt', 'r') as myfile: # python\n",
    "#with open('./java-developer-req1.txt', 'r') as myfile: # java\n",
    "#with open('./sql-req1.txt', 'r') as myfile: # SQL\n",
    "with open('./oracle-req1.txt', 'r') as myfile: # Oracle\n",
    "    test_raw=myfile.read().replace('\\n', ' ')\n",
    "    \n",
    "test_raw = test_raw.replace(\"\\x95\\t\",\" \")\n",
    "test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 POSTagging and keep nouns [no longer needed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Oracle Database Developer Softweb Solutions Inc - Parsippany , NJ - Must have strong Oracle PL/SQL application development skills\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(test_raw.decode('UTF-8'))\n",
    "print ' '.join(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Senior', 'JJ'),\n",
       " (u'Oracle', 'NNP'),\n",
       " (u'Database', 'NNP'),\n",
       " (u'Developer', 'NNP'),\n",
       " (u'Softweb', 'NNP'),\n",
       " (u'Solutions', 'NNP'),\n",
       " (u'Inc', 'NNP'),\n",
       " (u'-', ':'),\n",
       " (u'Parsippany', 'NN'),\n",
       " (u',', ','),\n",
       " (u'NJ', 'NNP')]"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_postag = nltk.pos_tag(tokens)\n",
    "input_postag[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle|Database|Developer|Softweb|Solutions|Inc|NJ|Oracle|PL/SQL|Stored|Procedures|Functions|Packages|Triggers|Views|Materialized|Views|Cursors|XML|SQL|Implementing|Stored|UNIX|CVS|Oracle|PL/SQL|Data|Integrity|Required|Skills|Software|Development|Oracle|RDBMS|SQL|OLTP|Agile|Methodology|Job|Type|Job|Location|NJ|Required|Bachelor|PL/SQL\n"
     ]
    }
   ],
   "source": [
    "# filtering list, keep NNP only\n",
    "input_postag_nn = [x[0] for x in input_postag if x[1] == 'NNP']\n",
    "#input_postag_nn = list(set(input_postag_nn))\n",
    "print '|'.join(input_postag_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification using Naive Bayes in scikit-learn \n",
    "[Working With Text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#working-with-text-data)  \n",
    "[Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)  \n",
    "[MultinomialNB Example](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train_l = [\" \".join(item) for item in d_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d_train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_nn = \" \".join(input_postag_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))#(stop_words = 'english')\n",
    "vectorizer_new = CountVectorizer(stop_words='english',ngram_range=(1, 2))#(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_vectorized = vectorizer.fit_transform(X_nn_summary_list)\n",
    "d_train_l_vector = vectorizer.fit_transform(d_train_l)\n",
    "\n",
    "# vectorize full text training data\n",
    "f_train_vector = vectorizer_new.fit_transform(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1271, 41473)\n",
      "1271\n",
      "(1071, 55508)\n",
      "1071\n"
     ]
    }
   ],
   "source": [
    "print d_train_l_vector.shape\n",
    "print len(d_target)\n",
    "print f_train_vector.shape\n",
    "print len(f_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25093"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_new.vocabulary_.get(u'java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(d_train_l_vector)\n",
    "#d_train_tf = tf_transformer.transform(d_train_l_vector)\n",
    "#d_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1271, 41473)\n",
      "1271 <type 'list'>\n",
      "(1071, 55508)\n",
      "1071 <type 'list'>\n",
      "1071\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer_new = TfidfTransformer()\n",
    "d_train_tfidf = tfidf_transformer.fit_transform(d_train_l_vector)\n",
    "print d_train_tfidf.shape\n",
    "print len(d_target),type(d_target)\n",
    "\n",
    "f_train_tfidf = tfidf_transformer_new.fit_transform(f_train_vector)\n",
    "print f_train_tfidf.shape\n",
    "print len(f_tech),type(f_tech)\n",
    "print len(f_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.633994172019005"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_train_tfidf.toarray()[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_nn = \" \".join(tokens)  # comment out this line if you want to use nnp instead of all test corpus\n",
    "# vectorize test data with nnp only\n",
    "input_nn_list = [input_nn]\n",
    "input_nn_vectorized = vectorizer.transform(input_nn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_raw to vector\n",
    "test_raw_list = [test_raw]\n",
    "test_raw_vectorized = vectorizer_new.transform(test_raw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test with data from training set\n",
    "testString = d_train_l[0].replace(\"\\xe2\\x80\\xa2\",\"\")\n",
    "testStringList = [testString]\n",
    "testVector = vectorizer.transform(testStringList)\n",
    "\n",
    "testString_new = f_train[0]\n",
    "testStringList_new = [testString_new]\n",
    "testVector_new = vectorizer_new.transform(testStringList_new)\n",
    "\n",
    "testVector2 = vectorizer.transform([\"This is a job requirement for python programmer\\\n",
    "    who needs to know flask, numpy and pandas.\"])\n",
    "testVector2_new = vectorizer_new.transform([\"This is a job requirement for python programmer\\\n",
    "    who needs to know python and python libraries such as flask, numpy and pandas.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL'] ['Python']\n",
      "['SQL'] ['Python']\n",
      "['Net Developer'] ['Python Developer']\n"
     ]
    }
   ],
   "source": [
    "# fit multinomial Naive Bayes model\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = MultinomialNB()\n",
    "clf3 = MultinomialNB()\n",
    "\n",
    "model1 = clf1.fit(d_train_l_vector, d_target) # TECH model\n",
    "# validate model using training data\n",
    "print model1.predict(testVector),model1.predict(testVector2) # Python # SQL\n",
    "\n",
    "model2 = clf2.fit(d_train_tfidf, d_target) # TECH model w/ df-idf\n",
    "print model2.predict(testVector),model2.predict(testVector2) # Python # SQL\n",
    "\n",
    "model3 = clf3.fit(d_train_tfidf, d_title) # TITILE model w/ df-idf\n",
    "print model3.predict(testVector),model3.predict(testVector2) # Python # SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1-Tech1: ['PL/SQL']\n",
      "Model2-Tech2: ['PL/SQL']\n",
      "Model3-Title: ['Oracle PL/SQL Developer']\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "print \"Model1-Tech1:\", model1.predict(input_nn_vectorized)\n",
    "print \"Model2-Tech2:\", model2.predict(input_nn_vectorized)\n",
    "print \"Model3-Title:\", model3.predict(input_nn_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Oracle | Oracle|Database|Developer|Softweb|Solutions|Inc|NJ|Oracle|PL/SQL|Stored|Procedures|Functions|Packages|Triggers|Views|Materialized|Views|Cursors|XML|SQL|Implementing|Stored|UNIX|CVS|Oracle|PL/SQL|Data|Integrity|Required|Skills|Software|Development|Oracle|RDBMS|SQL|OLTP|Agile|Methodology|Job|Type|Job|Location|NJ|Required|Bachelor|PL/SQL\n"
     ]
    }
   ],
   "source": [
    "# test data, 2grams is not shown here. 2grams are generated when calling vectorizer\n",
    "print \" \".join(tokens[:2]), \"|\",\"|\".join(input_nn.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit multinomial Naive Bayes model\n",
    "clf1_new = MultinomialNB()\n",
    "clf2_new = MultinomialNB()\n",
    "clf3_new = MultinomialNB()\n",
    "\n",
    "model1_new = clf1_new.fit(f_train_vector, f_tech) # TECH model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SQL'] [u'Linux']\n",
      "[u'SQL'] [u'Linux']\n",
      "[u'Net Developer'] [u'Linux Administrator']\n"
     ]
    }
   ],
   "source": [
    "# validate model using training data\n",
    "print model1_new.predict(testVector_new),model1_new.predict(testVector2_new) # Python # SQL\n",
    "\n",
    "model2_new = clf2_new.fit(f_train_tfidf, f_tech) # TECH model w/ df-idf\n",
    "print model2_new.predict(testVector_new),model2_new.predict(testVector2_new) # Python # SQL\n",
    "\n",
    "model3_new = clf3_new.fit(f_train_tfidf, f_title) # TITILE model w/ df-idf\n",
    "print model3_new.predict(testVector_new),model3_new.predict(testVector2_new) # Python # SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1_new-Tech1: [u'SQL']\n",
      "Model2_new-Tech2: [u'SQL']\n",
      "Model3_new-Title: [u'Net Developer']\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "print \"Model1_new-Tech1:\", model1_new.predict(test_raw_vectorized)\n",
    "print \"Model2_new-Tech2:\", model2_new.predict(test_raw_vectorized)\n",
    "print \"Model3_new-Title:\", model3_new.predict(test_raw_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest1 = RandomForestClassifier(n_estimators = 100) #random_state=15326\n",
    "forest2 = RandomForestClassifier(n_estimators = 100)\n",
    "forest3 = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest1 = forest1.fit(f_train_vector, f_tech)\n",
    "forest2 = forest2.fit(f_train_tfidf, f_tech)\n",
    "forest3 = forest3.fit(f_train_tfidf, f_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SQL'] [u'CSS']\n"
     ]
    }
   ],
   "source": [
    "# validate model with data from training set\n",
    "print forest1.predict(testVector_new), forest1.predict(testVector2_new)  #SQL Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest1-Tech1: [u'SQL']\n",
      "Forest2-Tech2: [u'Oracle']\n",
      "Forest3-Title: [u'Oracle DBA']\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "print \"Forest1-Tech1:\",forest1.predict(test_raw_vectorized)\n",
    "print \"Forest2-Tech2:\",forest2.predict(test_raw_vectorized)\n",
    "print \"Forest3-Title:\",forest3.predict(test_raw_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Senior Oracle Database Developer  Softweb Solutions Inc - Parsippany, NJ - Must have strong Oracle PL/SQL application development skills and excellent knowledge of Stored Procedures, Functions, Packages, Triggers, Views,Materialized Views, Cursors, and XML features and writing and tuning complex SQL queries. - Additionally, experience with autonomous transactions and error handling is highly desirable. - Experience in Implementing data models and database designs - Experienced with Stored procedure creation/debugging/ optimization/tuning - Experience with UNIX shell and CVS is helpful - Experience with Oracle 12c is desirable - Writing and creating data load scripts and stored procedures; ensuring that data is loaded in a timely manner daily - Working with application developers to assist in database design, query tuning, index assignment, and trigger and procedure creation - Ability to research and trouble-shoot application problems using PL/SQL programming skills - Ensuring that the integrity of company databases is maintained - Actively helping our Data Integrity team reconcile data - Excellent communication skills; works well in a team environment  Required Skills / Competencies: - 5-8+ years Software Development experience - 5-8+ years Designing and developing applications on Oracle RDBMS - 5+ years of experience in complex stored procedure creation and using performance tuning techniques - 4+ years of experience in creating and maintaining SQL code for medium to large OLTP applications  Agile Methodology programming experience is a plus  Job Type: Full-time  Job Location:  Parsippany, NJ Required education:  Bachelor's Required experience:  Oracle PL/SQL: 5 years  \""
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
