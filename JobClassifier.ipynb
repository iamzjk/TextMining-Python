{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputfile = \"./tags/nnp-summary.txt\"\n",
    "#nn_summary = \".\\\\tags\\\\nn-summary.txt\" # windows path\n",
    "\n",
    "#df_nn_summary = pd.read_table(nn_summary, sep=\"|\",index_col=False, header=None, error_bad_lines=False)\n",
    "\n",
    "#names = [\"token\"]*1314\n",
    "#df_nn_summary = pd.read_table(nn_summary, sep=\"|\",index_col=False, header=None, names = names) #error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(inputfile) as f:\n",
    "    reader = csv.reader(f, delimiter=\"|\")\n",
    "    d = list(reader)\n",
    "print d[0][2] # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL|SQL Developer|ETL|SSIS|SQL|Server|SQL|Server|Web|JavaScript|JQuery|HTML5|CSS3|JavaScript|•|Extensive|Microsoft|Business|Intelligence|SQL|Server|Integration|Services|SSIS|SQL|Server|Reporting|Services|SSRS|ETL|SSIS|Custom|Report|Tabular|Reports|Matrix|Reports|Ad|SQL|Server|Reporting|Services|SSRS|DATA|•|Expert|Bulk|BCP|Data|Transformation|Services|DTS|SSIS|•|Expertise|SQL|Server|Data|Modeling|Star|Schema/Snowflake|Schema|Fact|Dimensions|Visio|Software|Development|Life|Cycle|SDLC|Agile|Methodology|Waterfall|ETL|SSIS|•|Extensive|SSIS|File|Transfer|Protocol|FTP|Secure|File|Transfer|Protocol|SFTP\n"
     ]
    }
   ],
   "source": [
    "# the first line of our data\n",
    "print '|'.join(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL', 'SQL']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# TECH category list\n",
    "d_target = [item[0] for item in d]\n",
    "print d_target[0:10]\n",
    "print len(d_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL Developer', 'SQL Developer', 'SQL / BI Developer', 'Sr. SQL Developer', 'Sr. SQL Server DBA', 'Sr. SQL Developer', 'PL/SQL Developer', 'Sr. SQL Server DBA', 'SQL Server DBA', 'Sql Server/bi Developer']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# TITLE category list\n",
    "d_title = [' '.join(item[1:2]) for item in d]\n",
    "print d_title[0:10]\n",
    "print len(d_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ETL', 'SSIS', 'SQL', 'Server', 'SQL', 'Server', 'Web', 'JavaScript', 'JQuery', 'HTML5']\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "# token list\n",
    "d_train = [item[2:] for item in d]\n",
    "print d_train[0][:10]\n",
    "print len(d_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input data : job requirement.docx/doc\n",
    "\n",
    "Python_job_req1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STEP1: read in txt\n",
    "#with open('./Python_job_req1.txt', 'r') as myfile: # python\n",
    "#with open('./java-developer-req1.txt', 'r') as myfile: # java\n",
    "with open('./sql-req1.txt', 'r') as myfile: # SQL\n",
    "    text=myfile.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text = text.replace(\"\\x95\\t\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Developer Indus Software Solutions Inc. - Pennington , NJ $ 70 an hour - Contract Job Title : SQL\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text.decode('UTF-8'))\n",
    "print ' '.join(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'SQL', 'NNP'),\n",
       " (u'Developer', 'NNP'),\n",
       " (u'Indus', 'NNP'),\n",
       " (u'Software', 'NNP'),\n",
       " (u'Solutions', 'NNP'),\n",
       " (u'Inc.', 'NNP'),\n",
       " (u'-', ':'),\n",
       " (u'Pennington', 'NNP'),\n",
       " (u',', ','),\n",
       " (u'NJ', 'NNP'),\n",
       " (u'$', '$')]"
      ]
     },
     "execution_count": 1468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_postag = nltk.pos_tag(tokens)\n",
    "input_postag[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL|Developer|Indus|Software|Solutions|Inc.|Pennington|NJ|Job|Title|SQL|Developer|Location|Pennington|NJ|Duration|Pay|W2|MUST|WORK|ON|W2|NO|CORP|TO|CORP|Description|Bank|America|Sr.|SQL|Developer|Familiar|Skill|DB2|Informix|Tandem|SQL|DDBMS|Oracle|Teradata|RDBMS|Sybase|SQL|Server|Assembler|CICS|IMS|Network|Telecommunication|Applications|Security|Internet|Database|Wireless|Enterprise|Data|Warehousing|Teradata|RDBMS|Candidate|SQL|Server|R2|Cluster|Always|ON|Hrinish|Patalay|Indus|Software|Solutions|Inc|C|D|Job|Type|Salary|Bachelor|SQL|Developer|IT\n"
     ]
    }
   ],
   "source": [
    "# filtering list, keep NNP only\n",
    "input_postag_nn = [x[0] for x in input_postag if x[1] == 'NNP']\n",
    "#input_postag_nn = list(set(input_postag_nn))\n",
    "print '|'.join(input_postag_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate bigram for test data\n",
    "#def ngrams(input, n):\n",
    "#  output = []\n",
    "#  for i in range(len(input)-n+1):\n",
    "#    output.append(input[i:i+n])\n",
    "#  return output\n",
    "\n",
    "#input_postag_2grams = [' '.join(x) for x in ngrams(input_postag_nn,2)]\n",
    "#input_postag_nn = input_postag_nn + input_postag_2grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Naive Bayes in scikit-learn \n",
    "[Working With Text](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#working-with-text-data)  \n",
    "[Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)  \n",
    "[MultinomialNB Example](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train_l = [\" \".join(item) for item in d_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d_train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_nn = \" \".join(input_postag_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))#(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_vectorized = vectorizer.fit_transform(X_nn_summary_list)\n",
    "d_train_l_vector = vectorizer.fit_transform(d_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1271, 41473)\n",
      "1271\n"
     ]
    }
   ],
   "source": [
    "print d_train_l_vector.shape\n",
    "print len(d_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28170"
      ]
     },
     "execution_count": 1479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(u'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(d_train_l_vector)\n",
    "#d_train_tf = tf_transformer.transform(d_train_l_vector)\n",
    "#d_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1271, 41473)\n",
      "1271 <type 'list'>\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "d_train_tfidf = tfidf_transformer.fit_transform(d_train_l_vector)\n",
    "print d_train_tfidf.shape\n",
    "print len(d_target),type(d_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 1482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train_l_vector.toarray()[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_nn = \" \".join(tokens)  # comment out this line if you want to use nnp instead of all test corpus\n",
    "# vectorize test data with nnp only\n",
    "input_nn_list = [input_nn]\n",
    "input_nn_vectorized = vectorizer.transform(input_nn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test with data from training set\n",
    "testString = d_train_l[0].replace(\"\\xe2\\x80\\xa2\",\"\")\n",
    "testStringList = [testString]\n",
    "testVector = vectorizer.transform(testStringList)\n",
    "testVector2 = vectorizer.transform([\"This is a job requirement for python programmer\\\n",
    "    who needs to know flask, numpy and pandas.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL'] ['Python']\n",
      "['SQL'] ['Python']\n",
      "['Net Developer'] ['Python Developer']\n"
     ]
    }
   ],
   "source": [
    "# fit multinomial Naive Bayes model\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = MultinomialNB()\n",
    "clf3 = MultinomialNB()\n",
    "\n",
    "model1 = clf1.fit(d_train_l_vector, d_target) # TECH model\n",
    "# validate model using training data\n",
    "print model1.predict(testVector),model1.predict(testVector2) # Python # SQL\n",
    "\n",
    "model2 = clf2.fit(d_train_tfidf, d_target) # TECH model w/ df-idf\n",
    "print model2.predict(testVector),model2.predict(testVector2) # Python # SQL\n",
    "\n",
    "model3 = clf3.fit(d_train_tfidf, d_title) # TITILE model w/ df-idf\n",
    "print model3.predict(testVector),model3.predict(testVector2) # Python # SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1-Tech1: ['ETL']\n",
      "Model2-Tech2: ['PL/SQL']\n",
      "Model3-Title: ['Oracle PL/SQL Developer']\n"
     ]
    }
   ],
   "source": [
    "# predict test data\n",
    "print \"Model1-Tech1:\", model1.predict(input_nn_vectorized)\n",
    "print \"Model2-Tech2:\", model2.predict(input_nn_vectorized)\n",
    "print \"Model3-Title:\", model3.predict(input_nn_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Developer | SQL|Developer|Indus|Software|Solutions|Inc.|Pennington|NJ|Job|Title|SQL|Developer|Location|Pennington|NJ|Duration|Pay|W2|MUST|WORK|ON|W2|NO|CORP|TO|CORP|Description|Bank|America|Sr.|SQL|Developer|Familiar|Skill|DB2|Informix|Tandem|SQL|DDBMS|Oracle|Teradata|RDBMS|Sybase|SQL|Server|Assembler|CICS|IMS|Network|Telecommunication|Applications|Security|Internet|Database|Wireless|Enterprise|Data|Warehousing|Teradata|RDBMS|Candidate|SQL|Server|R2|Cluster|Always|ON|Hrinish|Patalay|Indus|Software|Solutions|Inc|C|D|Job|Type|Salary|Bachelor|SQL|Developer|IT\n"
     ]
    }
   ],
   "source": [
    "# test data, 2grams is not shown here. 2grams are generated when calling vectorizer\n",
    "print \" \".join(tokens[:2]), \"|\",\"|\".join(input_nn.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiying using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest1 = RandomForestClassifier(n_estimators = 100) #random_state=15326\n",
    "forest2 = RandomForestClassifier(n_estimators = 100)\n",
    "forest3 = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest1 = forest1.fit(d_train_l_vector, d_target)\n",
    "forest2 = forest2.fit(d_train_tfidf, d_target)\n",
    "forest3 = forest3.fit(d_train_tfidf, d_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL'] ['Python']\n"
     ]
    }
   ],
   "source": [
    "# validate model with data from training set\n",
    "print forest1.predict(testVector), forest1.predict(testVector2)  #SQL Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest1-Tech1: ['ETL']\n",
      "Forest2-Tech2: ['ETL']\n",
      "Forest3-Title: ['Premier SQL Support Engineer At Microsoft']\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "print \"Forest1-Tech1:\",forest1.predict(input_nn_vectorized)\n",
    "print \"Forest2-Tech2:\",forest2.predict(input_nn_vectorized)\n",
    "print \"Forest3-Title:\",forest3.predict(input_nn_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
